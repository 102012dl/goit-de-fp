Conclusion 

Нижче — спільні висновки та рекомендації щодо виконання обох частин фінального проєкту (Streaming Pipeline та Batch Data Lake), засновані на аналізі реалізацій, помилок і способів їх виправлення.

---

## Частина 1. End‑to‑End Streaming Pipeline (Kafka + PySpark + MySQL)

### Що спрацювало добре
- **Архітектура “Spark ← JDBC ← MySQL” та “Spark → Kafka → Spark Streaming”** була реалізована відповідно до завдання.
- Використання **Structured Streaming** із функцією `forEachBatch` для запису в Kafka та MySQL одночасно.
- Чітке розділення коду на етапи (1–6), з коментарями `# Етап X`.
- Написано інструкції для локального запуску в PyCharm / терміналі macOS та для Docker.

### Основні проблеми й як їх вирішили
1. **Помилки залежностей Spark–Kafka**  
   ‒ `AnalysisException: Failed to find data source: kafka` → потрібно додавати `spark‑packages` або `--packages org.apache.spark:spark-sql-kafka-0-10_2.12:…` при запуску.  
   **Рекомендація:** Завжди вказувати опцію `--packages` у `spark-submit` або в `SparkSession.builder.config("spark.jars.packages", …)`.

2. **Нестабільність FTP‑з’єднання**  
   ‒ Таймаути, “Max retries exceeded” → реалізовано retry‑механізм із `requests.Session` + `Retry`.  
   **Рекомендація:** Використовувати backoff і ретраї, перевіряти доступність URL перед основним запуском.

3. **Підключення до MySQL**  
   ‒ Помилки 2003 (HY000) “Can’t connect…” → локально MySQL не слухав `/tmp/mysql.sock`, контейнер мав іншу адресу.  
   **Рекомендація:** У Docker-оточенні використовувати `host.docker.internal` або мережеве ім’я контейнера, забезпечити health‑check контейнера перед запуском пайплайнів.

4. **Створення і наповнення таблиць**  
   ‒ Помилка 1287 “VALUES function is deprecated” → скориговано SQL‑синтаксис ON DUPLICATE KEY UPDATE із використанням псевдоніма `AS alias`.  
   **Рекомендація:** Тестувати DDL та DML у чистому MySQL Workbench перед інтеграцією в код, уникати застарілих конструкцій.

---

## Частина 2. Batch Data Lake (FTP + Spark + Airflow/Prefect)

### Що спрацювало добре
- Побудовано **multi‑hop архітектуру**: landing → bronze → silver → gold.
- Використано чисті, окремі скрипти (`landing_to_bronze.py`, `bronze_to_silver.py`, `silver_to_gold.py`) та DAG (Airflow) або Flow (Prefect).
- Налагоджено автоматизацію завантаження, перетворення та збереження даних у форматі Parquet.
- Додано перевірки існування директорій, ретельну обробку помилок, контроль схеми даних.

### Основні проблеми й як їх вирішили
1. **FTP‑завантаження**  
   ‒ Як і в Streaming‑частині, критично важливі retry і великі таймаути.  
   **Рекомендація:** Використовувати розгортання поблизу джерела даних (наприклад, у тій самій мережі) або сховище S3, щоб зменшити затримки.

2. **Оркестрація**  
   ‒ Airflow: складно налаштувати локально без Docker Compose. Prefect: конфлікти залежностей SQLAlchemy.  
   **Рекомендація:**  
   - Використати **Docker Compose** для всієї платформи (MySQL, Kafka, Zookeeper, Spark, Airflow/Prefect) — один `docker-compose.yml` для відтворюваності.  
   - Якщо обираєте Prefect, краще окреме середовище без Airflow (щоб уникнути конфліктів бібліотек).

3. **Тестування і валідація**  
   ‒ Не завжди було зрозуміло, чи правильно працює кожен шар (басейн bronze/silver/gold).  
   **Рекомендація:**  
   - Писати **unit‑тести** для окремих функцій трансформації (clean_text, deduplication, агрегація).  
   - Використовувати невеликі sample‑набори CSV із відомими результатами.  
   - Використовувати `pytest` або `unittest` у CI.

---

## Загальні рекомендації для обох частин

1. **Контейнеризація**  
   ― Виконуйте Spark, Kafka, MySQL, Airflow/Prefect у Docker для єдиного контрольованого оточення.  
2. **Інфраструктура як код**  
   ― Зберігайте `docker-compose.yml`, `requirements.txt` і всі DAG/Flow‑скрипти в репозиторії.  
3. **CI/CD**  
   ― Налаштуйте GitHub Actions або GitLab CI для автоматичного lint‑інгу, тестів і навіть запуску локального Spark‑job із sample‑даними.  
4. **Моніторинг і логування**  
   ― Використовуйте Prometheus/Grafana або Prefect UI для відстеження стану завдань.  
5. **Документація**  
   ― Описуйте порядок запуску та конфігурації в `README.md` проєкту, включно з інструкціями для macOS, Linux і Docker.

---

### Підсумок

Pеалізація охоплює всі ключові етапи обох завдань, але зіткнулася зі стандартними для Data Engineering труднощами: налаштування мереж, залежностей та обробки зовнішніх ресурсів. Використовуючи вищенаведені рекомендації з контейнеризації, оркестрації, тестування та моніторингу, ви зможете зробити ваші пайплайни більш надійними, відтворюваними та простими в супроводі.




